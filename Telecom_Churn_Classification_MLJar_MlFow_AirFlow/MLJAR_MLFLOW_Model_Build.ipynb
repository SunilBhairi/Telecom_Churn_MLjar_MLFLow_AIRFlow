{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7p/jmjy_99j6bx7_xbf0t58gz2m0000gn/T/ipykernel_32285/1432343492.py:56: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_train['total_rech_amt_data_6'] = df_train.av_rech_amt_data_6 * df_train.total_rech_data_6\n",
      "/var/folders/7p/jmjy_99j6bx7_xbf0t58gz2m0000gn/T/ipykernel_32285/1432343492.py:57: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_train['total_rech_amt_data_7'] = df_train.av_rech_amt_data_7 * df_train.total_rech_data_7\n",
      "/var/folders/7p/jmjy_99j6bx7_xbf0t58gz2m0000gn/T/ipykernel_32285/1432343492.py:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_train['total_rech_amt_data_8'] = df_train.av_rech_amt_data_8 * df_train.total_rech_data_8\n",
      "/var/folders/7p/jmjy_99j6bx7_xbf0t58gz2m0000gn/T/ipykernel_32285/1432343492.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test['total_rech_amt_data_6'] = df_test.av_rech_amt_data_6 * df_test.total_rech_data_6\n",
      "/var/folders/7p/jmjy_99j6bx7_xbf0t58gz2m0000gn/T/ipykernel_32285/1432343492.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test['total_rech_amt_data_7'] = df_test.av_rech_amt_data_7 * df_test.total_rech_data_7\n",
      "/var/folders/7p/jmjy_99j6bx7_xbf0t58gz2m0000gn/T/ipykernel_32285/1432343492.py:62: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test['total_rech_amt_data_8'] = df_test.av_rech_amt_data_8 * df_test.total_rech_data_8\n",
      "/var/folders/7p/jmjy_99j6bx7_xbf0t58gz2m0000gn/T/ipykernel_32285/1432343492.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_train['total_avg_rech_amt_6_7'] = (df_train.total_rech_amt_6 + df_train.total_rech_amt_data_6 + df_train.total_rech_amt_7+ df_train.total_rech_amt_data_7)/2\n",
      "/var/folders/7p/jmjy_99j6bx7_xbf0t58gz2m0000gn/T/ipykernel_32285/1432343492.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test['total_avg_rech_amt_6_7'] = (df_test.total_rech_amt_6 + df_test.total_rech_amt_data_6 + df_test.total_rech_amt_7+ df_test.total_rech_amt_data_7)/2\n"
     ]
    }
   ],
   "source": [
    "### MLJar for Telecom Churn project\n",
    "\n",
    "## Proprocesing steps\n",
    "\n",
    "#Data Structures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Sklearn\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "\n",
    "#Load the data\n",
    "df_train = pd.read_csv(\"/Users/I353375/Downloads/MLOps/airflow/Telecom_Churn/telecom_train.csv\")\n",
    "df_test = pd.read_csv(\"/Users/I353375/Downloads/MLOps/airflow/Telecom_Churn/telecom_test.csv\")\n",
    "sample = pd.read_csv(\"/Users/I353375/Downloads/MLOps/airflow/Telecom_Churn/telecom_sample.csv\")\n",
    "data_dict = pd.read_csv(\"/Users/I353375/Downloads/MLOps/airflow/Telecom_Churn/telecom_data_dictionary.csv\")\n",
    "\n",
    "#impute with zeroes on both df_train and df_test\n",
    "nullvalues=df_train.isnull().mean()*100 \n",
    "nullreplacezero_columns_train =  df_train.columns[df_train.columns.str.contains('_amt|count_|fb_user|_pck|total_|max_|_mou|_others')]\n",
    "nullreplacezero_columns_test =  df_test.columns[df_test.columns.str.contains('_amt|count_|fb_user|_pck|total_|max_|_mou|_others')]\n",
    "\n",
    "#After looking at the min values and other desrciptive stats about each of the columns \n",
    "# we can be certain about replacing NAs with zeroes in these columns\n",
    "for col in nullreplacezero_columns_train:\n",
    "    df_train[col] = df_train[col].replace(np.NaN,0.0)\n",
    "    \n",
    "for col in nullreplacezero_columns_test:\n",
    "    df_test[col] = df_test[col].replace(np.NaN,0.0)\n",
    "\n",
    "# getting rid of columns with just 1 unique value\n",
    "single_val_cols_train = df_train.columns[df_train.nunique() == 1]\n",
    "df_train.drop(single_val_cols_train,axis=1,inplace=True)\n",
    "\n",
    "# getting rid of columns with just 1 unique value\n",
    "single_val_cols_test = df_test.columns[df_test.nunique() == 1]\n",
    "df_test.drop(single_val_cols_test,axis=1,inplace=True)\n",
    "\n",
    "# Removing date columns because these wouldn't give any valuable insights about churn.\n",
    "date_columns =  df_train.columns[df_train.columns.str.contains('date')]\n",
    "df_train.drop(date_columns,axis=1,inplace=True)\n",
    "df_test.drop(date_columns,axis=1,inplace=True)\n",
    "\n",
    "# #Dropping the arpu columns because nullvalues are percentages are > 70%, and there is no correct way to impute these\n",
    "# df_train.drop((nullvalues[nullvalues > 0].index).tolist(),axis=1, inplace= True)\n",
    "# df_test.drop((nullvalues[nullvalues > 0].index).tolist(),axis=1, inplace= True)\n",
    "\n",
    "# new column Total_Recharge_Amount_Data = av_rech_amt*total_rech_data for each month\n",
    "df_train['total_rech_amt_data_6'] = df_train.av_rech_amt_data_6 * df_train.total_rech_data_6\n",
    "df_train['total_rech_amt_data_7'] = df_train.av_rech_amt_data_7 * df_train.total_rech_data_7\n",
    "df_train['total_rech_amt_data_8'] = df_train.av_rech_amt_data_8 * df_train.total_rech_data_8\n",
    "\n",
    "df_test['total_rech_amt_data_6'] = df_test.av_rech_amt_data_6 * df_test.total_rech_data_6\n",
    "df_test['total_rech_amt_data_7'] = df_test.av_rech_amt_data_7 * df_test.total_rech_data_7\n",
    "df_test['total_rech_amt_data_8'] = df_test.av_rech_amt_data_8 * df_test.total_rech_data_8\n",
    "\n",
    "df_train['total_avg_rech_amt_6_7'] = (df_train.total_rech_amt_6 + df_train.total_rech_amt_data_6 + df_train.total_rech_amt_7+ df_train.total_rech_amt_data_7)/2\n",
    "df_test['total_avg_rech_amt_6_7'] = (df_test.total_rech_amt_6 + df_test.total_rech_amt_data_6 + df_test.total_rech_amt_7+ df_test.total_rech_amt_data_7)/2\n",
    "\n",
    "## Some more columns to delete\n",
    "list_total_ic_cols = df_train.columns[df_train.columns.str.contains('total_ic_mou|std_ic_mou|loc_ic_mou',regex=True)]\n",
    "df_train.drop(list_total_ic_cols,axis=1,inplace=True)\n",
    "list_total_ic_cols.tolist()\n",
    "\n",
    "list_total_ic_cols = df_test.columns[df_test.columns.str.contains('total_ic_mou|std_ic_mou|loc_ic_mou',regex=True)]\n",
    "df_test.drop(list_total_ic_cols,axis=1,inplace=True)\n",
    "list_total_ic_cols.tolist()\n",
    "\n",
    "list_total_og_cols = df_train.columns[df_train.columns.str.contains('total_og_mou|std_og_mou|loc_og_mou',regex=True)]\n",
    "df_train.drop(list_total_og_cols,axis=1,inplace=True)\n",
    "list_total_og_cols.tolist()\n",
    "\n",
    "list_total_og_cols = df_test.columns[df_test.columns.str.contains('total_og_mou|std_og_mou|loc_og_mou',regex=True)]\n",
    "df_test.drop(list_total_og_cols,axis=1,inplace=True)\n",
    "list_total_og_cols.tolist()\n",
    "\n",
    "##handeling outliers\n",
    "\n",
    "def remove_outliers_iqr(df, column):\n",
    "    # Calculate Q1 (25th percentile) and Q3 (99th percentile)\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.99)\n",
    "    # Calculate IQR\n",
    "    IQR = Q3 - Q1  \n",
    "    # Calculate lower and upper bounds\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR   \n",
    "    # Filter the DataFrame\n",
    "    filtered_df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]   \n",
    "    return filtered_df\n",
    "\n",
    "df_train=remove_outliers_iqr(df_train,df_train.columns)\n",
    "\n",
    "df_test=remove_outliers_iqr(df_test,df_test.columns)\n",
    "\n",
    "## Apply the KNN Imputer to both train and test data to impute the Null values post removal of outliers \n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5)  \n",
    "imputer.fit(df_train)\n",
    "X_imputed = imputer.transform(df_train)\n",
    "\n",
    "df_train = pd.DataFrame(X_imputed, columns=df_train.columns)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)  \n",
    "imputer.fit(df_test)\n",
    "X_imputed = imputer.transform(df_test)\n",
    "\n",
    "df_test = pd.DataFrame(X_imputed, columns=df_test.columns)\n",
    "\n",
    "## Filtering High-Value Customers.\n",
    "\n",
    "high_value_filter = df_train.total_avg_rech_amt_6_7.quantile(0.7)\n",
    "high_val_cust_df_train = df_train[df_train.total_avg_rech_amt_6_7 > high_value_filter]\n",
    "\n",
    "high_value_filter = df_test.total_avg_rech_amt_6_7.quantile(0.7)\n",
    "high_val_cust_df_test = df_test[df_test.total_avg_rech_amt_6_7 > high_value_filter]\n",
    "\n",
    "df_train=high_val_cust_df_train.copy()\n",
    "\n",
    "df_unseen_test=df_test.copy()\n",
    "\n",
    "## MINMAX Scaling\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data2=df_train.drop(['id','churn_probability'],axis=1)\n",
    "data3=df_test.drop('id',axis=1)\n",
    "#initiate a object\n",
    "scaler=MinMaxScaler()\n",
    "\n",
    "## Create a list of Numerical vars\n",
    "var1=data2.columns\n",
    "var2=data3.columns\n",
    "#Fit the method\n",
    "df_train[var1]= scaler.fit_transform(df_train[var1])\n",
    "df_test[var2]= scaler.transform(df_test[var2])\n",
    "\n",
    "df_train['churn_probability']=df_train['churn_probability'].replace({'No churn': 0, 'Churn': 1})\n",
    "\n",
    "df_train.id = df_train.id.astype(int)\n",
    "df_train.churn_probability = df_train.churn_probability.astype(int)\n",
    "df_test.id = df_test.id.astype(int)\n",
    "\n",
    "df_unseen_test=df_test.copy()\n",
    "\n",
    "## Train test Split\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "df_train,df_test = train_test_split(df_train, train_size=0.7,random_state=100)\n",
    "\n",
    "y_train=df_train['churn_probability']\n",
    "X_train=df_train.drop(['id','churn_probability'],axis=1)\n",
    " \n",
    "y_test=df_test['churn_probability']\n",
    "X_test=df_test.drop(['id','churn_probability'],axis=1)\n",
    "\n",
    "## Lets use SMOTE to Remove the data imbalance\n",
    "from imblearn.datasets import make_imbalance\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# oversampling the train dataset using SMOTE\n",
    "smt = SMOTE()\n",
    "X_train, y_train = smt.fit_resample(X_train, y_train)\n",
    "X_test, y_test = smt.fit_resample(X_test, y_test)\n",
    "\n",
    "# Save files with compression for faster I/O\n",
    "X_train.to_csv('/Users/I353375/Downloads/MLOps/airflow/Telecom_Churn/X_train.csv', index=False)\n",
    "X_test.to_csv('/Users/I353375/Downloads/MLOps/airflow/Telecom_Churn/X_test.csv', index=False)\n",
    "y_train.to_csv('/Users/I353375/Downloads/MLOps/airflow/Telecom_Churn/y_train.csv', index=False)\n",
    "y_test.to_csv('/Users/I353375/Downloads/MLOps/airflow/Telecom_Churn/y_test.csv', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/27 09:17:26 INFO mlflow.tracking.fluent: Experiment with name 'Telecom_Churn_Classification' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear algorithm was disabled.\n",
      "AutoML directory: /Users/I353375/Downloads/MLOps/mlflow_server/mljar_classification/automl_results\n",
      "The task is binary_classification with evaluation metric logloss\n",
      "AutoML will use algorithms: ['Xgboost', 'LightGBM', 'CatBoost', 'Decision Tree', 'Extra Trees']\n",
      "AutoML will stack models\n",
      "AutoML will ensemble available models\n",
      "AutoML steps: ['adjust_validation', 'simple_algorithms', 'default_algorithms', 'not_so_random', 'golden_features', 'kmeans_features', 'insert_random_feature', 'features_selection', 'hill_climbing_1', 'hill_climbing_2', 'boost_on_errors', 'ensemble', 'stack', 'ensemble_stacked']\n",
      "* Step adjust_validation will try to check up to 1 model\n",
      "1_DecisionTree logloss 0.396727 trained in 1.42 seconds\n",
      "Adjust validation. Remove: 1_DecisionTree\n",
      "Validation strategy: 5-fold CV Shuffle,Stratify\n",
      "* Step simple_algorithms will try to check up to 3 models\n",
      "1_DecisionTree logloss 0.385269 trained in 4.79 seconds\n",
      "2_DecisionTree logloss 0.350166 trained in 5.01 seconds\n",
      "3_DecisionTree logloss 0.350166 trained in 5.11 seconds\n",
      "* Step default_algorithms will try to check up to 4 models\n",
      "4_Default_LightGBM logloss 0.066423 trained in 81.19 seconds\n",
      "5_Default_Xgboost logloss 0.067204 trained in 36.59 seconds\n",
      "6_Default_CatBoost logloss 0.073883 trained in 90.96 seconds\n",
      "* Step not_so_random will try to check up to 36 models\n",
      "16_LightGBM logloss 0.071312 trained in 64.42 seconds\n",
      "7_Xgboost logloss 0.069879 trained in 35.03 seconds\n",
      "Skip golden_features because of the time limit.\n",
      "* Step kmeans_features will try to check up to 3 models\n",
      "5_Default_Xgboost_KMeansFeatures logloss 0.068327 trained in 51.72 seconds\n",
      "Not enough time to perform features selection. Skip\n",
      "Time needed for features selection ~ 74.0 seconds\n",
      "Please increase total_time_limit to at least (800 seconds) to have features selection\n",
      "Skip insert_random_feature because no parameters were generated.\n",
      "Skip features_selection because no parameters were generated.\n",
      "* Step hill_climbing_1 will try to check up to 14 models\n",
      "17_LightGBM logloss 0.069233 trained in 63.72 seconds\n",
      "* Step hill_climbing_2 will try to check up to 12 models\n",
      "18_LightGBM logloss 0.068357 trained in 80.94 seconds\n",
      "Skip boost_on_errors because of the time limit.\n",
      "* Step ensemble will try to check up to 1 model\n",
      "Ensemble logloss 0.064109 trained in 2.28 seconds\n",
      "* Step stack will try to check up to 8 models\n",
      "4_Default_LightGBM_Stacked logloss 0.062371 trained in 22.7 seconds\n",
      "5_Default_Xgboost_Stacked logloss 0.058239 trained in 10.51 seconds\n",
      "6_Default_CatBoost_Stacked logloss 0.056536 trained in 14.49 seconds\n",
      "18_LightGBM_Stacked not trained. Stop training after the first fold. Time needed to train on the first fold 5.0 seconds. The time estimate for training on all folds is larger than total_time_limit.\n",
      "5_Default_Xgboost_KMeansFeatures_Stacked logloss 0.057888 trained in 18.32 seconds\n",
      "* Step ensemble_stacked will try to check up to 1 model\n",
      "Ensemble_Stacked logloss 0.055912 trained in 3.17 seconds\n",
      "AutoML fit time: 610.76 seconds\n",
      "AutoML best model: Ensemble_Stacked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'mljar_classification_model'.\n",
      "2025/04/27 09:28:17 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: mljar_classification_model, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Classification model training complete and logged to MLflow successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'mljar_classification_model'.\n"
     ]
    }
   ],
   "source": [
    "### -- model bulding using ML Jar One time run to save the model in MlFlow\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from supervised.automl import AutoML\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# --- Step 1: MLflow Setup ---\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"Telecom_Churn_Classification\")\n",
    "mlflow.autolog(disable=True)  # Manual logging\n",
    "\n",
    "# --- Define paths ---\n",
    "experiment_path = \"/Users/I353375/Downloads/MLOps/mlflow_server/mljar_classification/experiments\"\n",
    "model_path = \"/Users/I353375/Downloads/MLOps/mlflow_server/mljar_classification/models\"\n",
    "results_path = \"/Users/I353375/Downloads/MLOps/mlflow_server/mljar_classification/automl_results\"\n",
    "\n",
    "os.makedirs(experiment_path, exist_ok=True)\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "# --- Step 2: Load Data ---\n",
    "X_train = pd.read_csv('/Users/I353375/Downloads/MLOps/airflow/Telecom_Churn/X_train.csv')\n",
    "X_test = pd.read_csv('/Users/I353375/Downloads/MLOps/airflow/Telecom_Churn/X_test.csv')\n",
    "y_train = pd.read_csv('/Users/I353375/Downloads/MLOps/airflow/Telecom_Churn/y_train.csv')\n",
    "y_test = pd.read_csv('/Users/I353375/Downloads/MLOps/airflow/Telecom_Churn/y_test.csv')\n",
    "\n",
    "y_train.columns = ['churn_probability']\n",
    "y_test.columns = ['churn_probability']\n",
    "\n",
    "# --- Step 3: Train AutoML ---\n",
    "automl = AutoML(\n",
    "    mode=\"Compete\",\n",
    "    total_time_limit=600,\n",
    "    eval_metric=\"logloss\",  # üí° Typical metric for classification\n",
    "    explain_level=0,\n",
    "    algorithms=[\"Xgboost\", \"LightGBM\", \"CatBoost\", \"Linear\", \"Decision Tree\", \"Extra Trees\"],\n",
    "    ml_task=\"binary_classification\",  # üèÅ Classification mode\n",
    "    results_path=results_path\n",
    ")\n",
    "\n",
    "automl.fit(X_train, y_train)\n",
    "\n",
    "# --- Step 4: Predictions ---\n",
    "y_pred_test = automl.predict(X_test)\n",
    "y_pred_train = automl.predict(X_train)\n",
    "\n",
    "# --- Step 5: Metrics ---\n",
    "metrics = {\n",
    "    \"accuracy_test\": accuracy_score(y_test, y_pred_test),\n",
    "    \"precision_test\": precision_score(y_test, y_pred_test),\n",
    "    \"recall_test\": recall_score(y_test, y_pred_test),\n",
    "    \"f1_test\": f1_score(y_test, y_pred_test),\n",
    "    \"roc_auc_test\": roc_auc_score(y_test, y_pred_test),\n",
    "    \"accuracy_train\": accuracy_score(y_train, y_pred_train),\n",
    "    \"precision_train\": precision_score(y_train, y_pred_train),\n",
    "    \"recall_train\": recall_score(y_train, y_pred_train),\n",
    "    \"f1_train\": f1_score(y_train, y_pred_train),\n",
    "    \"roc_auc_train\": roc_auc_score(y_train, y_pred_train)\n",
    "}\n",
    "\n",
    "# --- Step 6: Define MLflow Pyfunc Wrapper ---\n",
    "class MLJARWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        from supervised.automl import AutoML\n",
    "        self.model = AutoML(results_path=results_path)\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        return self.model.predict(model_input)\n",
    "\n",
    "# --- Step 7: Save Model Folder ---\n",
    "shutil.copytree(\n",
    "    src=results_path,\n",
    "    dst=model_path,\n",
    "    dirs_exist_ok=True\n",
    ")\n",
    "\n",
    "# --- Step 8: Log into MLflow ---\n",
    "with mlflow.start_run(run_name=\"Telecom_Churn_Classification\"):\n",
    "    # Params\n",
    "    mlflow.log_params({\n",
    "        \"mode\": \"Compete\",\n",
    "        \"time_limit_secs\": 600,\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"task\": \"binary_classification\",\n",
    "        \"framework\": \"MLJAR\",\n",
    "    })\n",
    "\n",
    "    # Metrics\n",
    "    mlflow.log_metrics(metrics)\n",
    "\n",
    "    # Signature\n",
    "    signature = infer_signature(X_train, y_pred_train)\n",
    "\n",
    "    # Log model\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model\",\n",
    "        python_model=MLJARWrapper(),\n",
    "        signature=signature,\n",
    "        code_path=[],\n",
    "        registered_model_name=\"mljar_classification_model\"\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Classification model training complete and logged to MLflow successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
